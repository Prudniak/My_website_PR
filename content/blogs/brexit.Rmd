---
categories:  
- ""   
- ""
date: "2022-09-13"
description: Brexit vote analysis
draft: false
image: pic_p04.jpg 

keywords: ""
slug: brexit 
title: Brexit analysis
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```



```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(wbstats)
library(countrycode)
library(patchwork)
library(gganimate)
library(infer)
library(scales)
```

# Climate change and temperature anomalies

If we wanted to study climate change, we can find data on the *Combined
Land-Surface Air and Sea-Surface Water Temperature Anomalies* in the
Northern Hemisphere at [NASA's Goddard Institute for Space
Studies](https://data.giss.nasa.gov/gistemp). The [tabular data of
temperature anomalies can be found
here](https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt)

To define temperature anomalies you need to have a reference, or base,
period which NASA clearly states that it is the period between
1951-1980.

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

For each month and year, the dataframe shows the deviation of
temperature from the normal (expected). Further the dataframe is in wide
format.

**Cleaning Data**

```{r tidyweather}

tidyweather <- weather %>% 
  select(1:13) %>% 
  pivot_longer( cols = 2:13,
                names_to = "Month",
                values_to = "delta")

glimpse(tidyweather)

```

## Plotting Information

Plotting data using a time-series scatterplot with a trendline.

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies",
    x = "Year",
    y = "Temperature deviaton",
    caption = "Source: https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt"
  ) +
  NULL

```


Producing a scatter plot showing the temperature anomalies by month.


```{r facet_wrap}

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies",
    x = "Year",
    y = "Temperature deviaton",
    caption = "Source: https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt"
  ) +
  facet_wrap(~month) +
  NULL

```

Grouping data into different time periods to study historical data.

```{r intervals}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

comparison

```

Creating a density plot to study the distribution of monthly deviations
grouped by the different time periods.

```{r density_plot}

ggplot(data = comparison, aes(delta)) +
  geom_density(aes(fill = interval), alpha = 1/4) +
  labs(title = "Distribution of Monthly Temperature Anomalies in Time Intervals",
       x = "Monthly Temperature Anomaly", 
       y = "Density",
    caption = "Source: https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt") +
  facet_wrap(~ interval, ncol = 1) +
  theme_bw() +
  theme(legend.position = "none") +
  NULL

```

Calculating average annual anomalies.

```{r averaging}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  summarise(mean_delta = mean(delta, na.rm=TRUE))

#plotting the data:

ggplot(average_annual_anomaly, 
       aes (x = Year,
            y = mean_delta)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "loess") +
  labs(title = "Average annual anomalies by year",
       y = "Average annual anomalies",
    caption = "Source: https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt") +
  NULL

```

## Confidence Interval for `delta`

[NASA points out on their
website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php)
that

> A one-degree global change is significant because it takes a vast
> amount of heat to warm all the oceans, atmosphere, and land by that
> much. In the past, a one- to two-degree drop was all it took to plunge
> the Earth into the Little Ice Age.

Construction of a confidence interval for the average annual delta since
2011, both using a formula and using a bootstrap simulation with the
`infer` package.

```{r, calculate_CI_using_formula}

formula_ci <- comparison %>% 
  filter(interval == "2011-present") %>% # choose the interval 2011-present
  filter(!delta == "NA") %>% # drop NA observations in delta
  summarise(count = n(),
            t = qt(0.975, count-1), # use qt with probability and degrees of freedom
            mean = mean(delta), # calculate mean
            sd = sd(delta), # calculate sd
            se = sd(delta)/sqrt(count), # calculate se
            margin = t * se, # calculate margin of error
            lower = mean - margin, # calculate lower bound
            upper = mean + margin #calculate upper bound
            
  )

#print out formula_CI
formula_ci
```

```{r, calculate_CI_bootstrap}
library(infer)

set.seed(1234)

boot_ci <- comparison %>% 
  filter(interval == "2011-present") %>% # choose the interval 2011-present
  filter(!delta == "NA") %>% # drop NA observations in delta
  specify(response = delta) %>% # specify the variable of interest
  generate(reps = 1000, type = "bootstrap") %>% # extract 1000 bootstrap samples
  calculate(stat = "mean") %>% # calculate sample means from each bootstrap sample
  get_confidence_interval(level = 0.95, type = "percentile") # calculate confidence interval of this analysis

# Display confidence interval
boot_ci
```

> Two different methods of constructing 95% confidence interval were
> used in this example. First was based on filtering appropriate
> interval and calculating confidence interval using summary statistics.
> Second involved 'infer' package, which allowed to use bootstrap method
> and produced the confidence intervals without any additional summary
> statistics. According to the summary calculations the average annual
> anomalies since 2011 already exceeded 1 degree, even when 95%
> confidence interval is taken into account. Therefore, it is highly
> likely that anomalies will become even more frequent and significant
> than before.

# Biden's Approval Margins

As we saw in class, fivethirtyeight.com has detailed data on [all polls
that track the president's
approval](https://projects.fivethirtyeight.com/biden-approval-ratings)

```{r, cache=TRUE}
# Import approval polls data directly off fivethirtyeight website
approval_pollist <- read_csv('https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv') 

#fixing dates using 'lubridate' package
approval_pollist <- approval_pollist %>% 
  mutate(modeldate = mdy(modeldate),
         startdate = mdy(startdate),
         enddate = mdy(enddate),
         year = year(enddate),
         week_number = week(enddate))

glimpse(approval_pollist)

```

## Create a plot

Calculating the average net approval rate (approve- disapprove) for each
week since Biden got into office and plotting the net approval for each
week in 2022, along with its 95% confidence interval.

```{r trump_margins, out.width="100%"}
biden_plot <- approval_pollist %>% 
  
  # filtering 2007 year
  filter(year == "2022") %>% 
  
  # adding new column with difference between approve and disapprove
  mutate(net_approval = approve - disapprove) %>% 
  
  # grouping by subgroup and number of the week
  group_by(subgroup, week_number)  %>% 
  
  # calculating summary statistics needed for 95% confidence interval
  summarise(mean_net = mean(net_approval), #mean
            sd_net = sd(net_approval), #standard devation
            count = n(), #number of observations
            t_critical = qt(0.975, count-1), #t statistic
            se_net = sd_net / sqrt(count), #standard error
            margin_of_error = t_critical * se_net, #margin of error
            net_low = mean_net - margin_of_error, #lower part of confidence interval
            net_high = mean_net + margin_of_error) #upper part of confidence interval


# Creating plot
ggplot(biden_plot, 
       aes(x = week_number,
           y = mean_net,
           color = subgroup)) +
  geom_line(size = 1.05) + 
  geom_ribbon(aes(ymin = net_low,
                  ymax = net_high,
                  alpha = 200),
              fill = "#F7E1AD",
              size = 1.05) +
  facet_grid(rows = vars(subgroup)) +
   labs(title = "Biden's Net Approval Ratings in 2022",
        subtitle = "Weekly Data, Approve - Disapprove, %",
        caption = "Source: https://projects.fivethirtyeight.com/biden-approval-data",
       x = "Week in 2022", 
       y = NULL) +
  theme_bw() +
  theme(legend.position = "none") +
  NULL



```

# Challenge 1: Excess rentals in TfL bike sharing

Loading data

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = week(day))
```

Creating a facet grid that plots bikes hired by month and year since
2015

```{r tfl_month_year_grid, out.width="100%"}

bike %>% 
  
  #filtering 2015-present
  filter(year >= 2015) %>% 
  
  #creating plot
  ggplot(aes(x = bikes_hired)) +
  geom_density() +
  facet_grid(rows = vars(year),
             cols = vars(month)) +
  labs(title = "Distribution of bikes hired per month",
       x = "Bike rentals", 
       y = NULL,
       caption = "Source: https://data.london.gov.uk/dataset/number-bicycle-hires") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.y = element_blank()) +
  scale_x_continuous(labels = label_number(suffix = "K", scale = 1e-3)) +
  NULL


```

Plotting monthly changes in TfL bike rentals.

```{r tfl_absolute_monthly_change, out.width="100%"}

#calculating average for 2016-2019 period
bike1 <- bike %>%
  filter(year %in% c("2016","2017","2018","2019")) %>%
  group_by(month) %>%
  summarise(monthly_hired = mean(bikes_hired))

#calculating means for 2017-2022 years
bike2 <- bike %>%
  filter(year %in% c(2017:2022)) %>%
  group_by(month,year) %>%
  summarise(mean_bikes_hired = mean(bikes_hired))

#merging two datasets
bike_coord1 <- merge(bike2, bike1, all.x=T)


#adding new column that chooses smaller value from the mean_bikes_hired & monthly_hired columns
bike_coord1 <- bike_coord1 %>% 
  mutate(min_1 = pmin(mean_bikes_hired,monthly_hired))
  
#creating the plot
ggplot(bike_coord1,aes(x=month,y=monthly_hired)) +
  geom_line(aes(group=1),
            color="#0000FF", 
            size = 1.2) +
  geom_line(data=bike_coord1,
            aes(month,mean_bikes_hired),
            group=1, 
            size = 1.05,
            color = "#000000") +
  geom_ribbon(aes(ymin=min_1, 
                  ymax = mean_bikes_hired, 
                  group=year), 
              fill = "#00CC00", 
              alpha = 0.4) +
  geom_ribbon(aes(ymin=min_1, 
                  ymax = monthly_hired, 
                  group=year), 
              fill = "#FF0000", 
              alpha = 0.4) +
  facet_wrap(~year) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Monthly changes in TfL bike rentals", 
       subtitle = "Change from monthly average shown in blue and calculated between 2016-2019", 
       x = "Months", 
       y = "Bike rentals",
       caption = "Source: https://data.london.gov.uk/dataset/number-bicycle-hires") +
  NULL


```

> Number of rented bikes in years 2017-2019 is close to the average
> calculated between 2016-2019. However, since 2020 clear influence of
> COVID pandemic and lockdowns is visible. First, in spring of 2020
> there is huge decline, as most people stayed at home. After initial
> lockdown more and more people decided to switch to bikes from other
> means of transportation. It seems that people that switch to bikes
> decided not to come back to their old habits after restrictions were
> lifted as bike rentals in 2022 in every month exceed average from
> 2016-2019.

Plotting percentage changes from the expected level of weekly rentals of
TfL bikes.

```{r tfl_percent_change, out.width="100%"}

#calculating weekly average for 2016-2019 period
bike3 <- bike %>%
  filter(year %in% c("2016","2017","2018","2019")) %>%
  group_by(week) %>%
  summarise(weekly_hired = mean(bikes_hired))

#calculating weekly means for 2017-2022 period
bike4 <- bike %>%
  filter(year %in% c(2017:2022)) %>%
  group_by(week,year) %>%
  summarise(mean_bikes_hired = mean(bikes_hired))


#merging two datasets
bike_coord2 <- merge(bike4, bike3, all.x=T)


bike_coord2 <- bike_coord2 %>% 
  #calculating percentage change
  mutate(percent_change=(mean_bikes_hired-weekly_hired)/weekly_hired,
         #adding two columns 1) with values lower than zero + 0, and 2) with values greater than zero + 0
         min_1=pmin(percent_change,0),
         max_1=pmax(percent_change,0),
         #adding class to values depending on whether the values are positive or negative
         class = as.factor(ifelse(percent_change >= 0, "positive", "negative")))
  

#creating plot
ggplot(bike_coord2,aes(x=week,y=percent_change)) +
   annotate("rect", 
           xmin = 14, 
           xmax = 26,
           ymin = -0.5, 
           ymax = 1,
           fill = "#E0E0E0", 
           alpha = 0.7) +
  annotate("rect", 
           xmin = 40, 
           xmax = 52,
           ymin = -0.5, 
           ymax = 1,
           fill = "#E0E0E0", 
           alpha = 0.7) +
  geom_line(aes(group=1),
            size = 1.05,
            color = "#000000") +
  geom_ribbon(aes(ymin=min_1,
                  ymax=percent_change,
                  group=year),
              fill="#00CC00", 
              alpha=0.25) +
  geom_ribbon(aes(ymin=percent_change, 
                  ymax=max_1,
                  group=year),
              fill="#FF0000", 
              alpha=0.25) +
  geom_rug(sides = "b",
           aes(color = class)) +
  scale_color_manual(values = c("#FF0000", "#00CC00")) +
  facet_wrap(~year) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Weekly changes in TfL bike rentals", 
       subtitle = "% change from weekly averages shown 
  calculated between 2016-2019", 
       x = "Weeks", 
       y = "Bike rentals",
       caption = "Source: https://data.london.gov.uk/dataset/number-bicycle-hires") +
  scale_y_continuous(labels = scales::percent) +
  NULL



```

> Weekly % changes supports conclusions from the first graph. Moreover,
> the impact of the lockdowns in spring of 2020 and winter of 2021 are
> much better visible.

# Challenge 2: Share of renewable energy production in the world

The National Bureau of Economic Research (NBER) has a a very interesting
dataset on the adoption of about 200 technologies in more than 150
countries since 1800. This is the[Cross-country Historical Adoption of
Technology (CHAT)
dataset](https://www.nber.org/research/data/cross-country-historical-adoption-technology).

The following is a description of the variables

| **variable** | **class** | **description**                |
|--------------|-----------|--------------------------------|
| variable     | character | Variable name                  |
| label        | character | Label for variable             |
| iso3c        | character | Country code                   |
| year         | double    | Year                           |
| group        | character | Group (consumption/production) |
| category     | character | Category                       |
| value        | double    | Value (related to label)       |

```{r,load_technology_data}

technology <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-19/technology.csv')

#get all technologies
labels <- technology %>% 
  distinct(variable, label)

# Get country names using 'countrycode' package
technology <- technology %>% 
  filter(iso3c != "XCD") %>% 
  mutate(iso3c = recode(iso3c, "ROM" = "ROU"),
         country = countrycode(iso3c, origin = "iso3c", destination = "country.name"),
         country = case_when(
           iso3c == "ANT" ~ "Netherlands Antilles",
           iso3c == "CSK" ~ "Czechoslovakia",
           iso3c == "XKX" ~ "Kosovo",
           TRUE           ~ country))

#make smaller dataframe on energy
energy <- technology %>% 
  filter(category == "Energy")

# download CO2 per capita from World Bank using {wbstats} package
# https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1970, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated))

# get a list of countries and their characteristics
# we just want to get the region a country is in and its income level
countries <-  wb_cachelist$countries %>% 
  select(iso3c,region,income_level)

```

Creating of a graph with the countries with the highest and lowest %
contribution of renewables in energy production.

```{r min-max_renewables, out.width="100%"}

energy <- energy %>% 
  
  #filteting out the NAs
  filter(!is.na(value))

#Top 20 countries with highest % contribution of renewables in energy production

top_res <- energy %>% 
  
  # dropping unnessecary columns
  select(-c(label, iso3c, group, category)) %>% 
  
  #pivoting dataset wider
  pivot_wider(names_from = "variable",
              values_from = "value") %>% 
  
  #filtering year 2019
  filter(year == 2019) %>% 
  
  #grouping by country
  group_by(country) %>% 
  
  #calculating the % of renewables in energy production
  summarise(total_res_perc = sum(elec_hydro, elec_solar, elec_wind, elec_renew_other)/ sum(elecprod)) %>% 
  
  #arranging in descending order 
  arrange(desc(total_res_perc)) %>% 
  
  #choosing top 20 countries
  head(20)


#Creating plot with Top 20 countries with highest % contribution of renewables in energy production

top_res_plot <- ggplot(top_res, 
       aes(x = total_res_perc, 
           y = fct_reorder(country, total_res_perc))) +
  geom_col() +
     labs(subtitle = "Highest",
       x = NULL, 
       y = NULL) +
  theme_light() +
  theme(legend.position = "none") +
  scale_x_continuous(labels = scales::percent) +
  NULL


#Top 20 countries with lowest % contribution of renewables in energy production

bot_res <- energy %>% 
  
  # dropping unnessecary columns
  select(-c(label, iso3c, group, category)) %>% 
  
  #pivoting dataset wider
  pivot_wider(names_from = "variable",
              values_from = "value") %>% 
  
  #filtering year 2019
  filter(year == 2019) %>% 
  
  #grouping by country
  group_by(country) %>% 
  
  #calculating the % of renewables in energy production
  summarise(total_res_perc = sum(elec_hydro, elec_solar, elec_wind, elec_renew_other)/ sum(elecprod)) %>% 
  
  #arranging in ascending order 
  arrange(total_res_perc) %>% 
  
  #choosing top 20 countries
  head(20)


#Creating plot with Top 20 countries with lowest % contribution of renewables in energy production

bot_res_plot <- ggplot(bot_res, 
       aes(x = total_res_perc, 
           y = fct_reorder(country, total_res_perc))) +
  geom_col() +
     labs(subtitle = "Lowest",
       x = NULL, 
       y = NULL) +
  theme_light() +
  theme(legend.position = "none") +
  scale_x_continuous(labels = scales::percent) +
  NULL



#joining two plots together with 'patchwork'

res_plot <- top_res_plot / bot_res_plot + 
  plot_annotation(title = "Highest and lowest % of renewables in energy production",
                  subtitle = "2019 data",
                  caption = "Source: NBER CHAT Database") 

res_plot


```

Creating an animation to explore the relationship between CO2 per capita
emissions and the deployment of renewables.

```{r animation, echo=FALSE, out.width="100%"}

#cleaning energy dataset
energy_1 <- energy %>% 
  
  #removing not needed columns
  select(-c(label, group, category)) %>% 
  
  #pivoting data wider
  pivot_wider(names_from = "variable",
              values_from = "value") %>% 
  
  #Left-joining data 
  left_join(y = countries, by = "iso3c") %>% 
  
  #removing not needed columns
  select(-c(region))


#cleaning emissions dataset
co2_percap_new <- co2_percap %>%  
  
  #renaming columns
  rename(CO2_emissions = "value",
         year = "date") %>%
  
  #selecting necessary columns
  select(iso3c, year, CO2_emissions) 

#Left-joining energy_1 and co2_percap_new datasets
energy_new <- left_join(energy_1, co2_percap_new, by = c("iso3c" = "iso3c", "year" = "year"))


```

```{r plotting}

energy_plot <- energy_new %>% 
  
  #filtering year & NAs
  filter(year >= 1991) %>% 
  filter(!is.na(income_level)) %>% 
  
  #grouping by country and year
  group_by(country, year, income_level) %>% 
  
  #calculating the % of renewables in energy production
  summarise(total_res_perc = sum(elec_hydro, elec_solar, elec_wind, elec_renew_other)/ sum(elecprod),
        emissions = CO2_emissions) 

#creating plot
p <- ggplot(energy_plot, 
       aes(x = total_res_perc,
           y = emissions,
           color = income_level)) +
  geom_point() +
  labs(title = 'Year: {as.integer(frame_time)}', 
           x = '% of renewables', 
           y = 'CO2 per cap',
       caption = "Source: NBER CHAT Database") +
  transition_time(year) +
  ease_aes('linear') +
  facet_wrap(~income_level, ncol = 2) +
  theme_bw() +
  theme(legend.position = "none") +
  scale_x_continuous(labels = scales::percent) +
  NULL

  
animate(p) 

```

> In every income group, it seems that % of renewables in energy
> production is negatively correlated with the amount of CO2 emitted per
> capita. Therefore, investing in such energy sources could be leveraged
> to achieve net zero strategies by countries all over the world.

# Deliverables

As usual, there is a lot of explanatory text, comments, etc. You do not
need these, so delete them and produce a stand-alone document that you
could share with someone. Knit the edited and completed R Markdown file
as an HTML document (use the "Knit" button at the top of the script
editor window) and upload it to Canvas.

# Details

-   Who did you collaborate with: Neha Dagade, Piotr Rudniak, Jomal
    Jochan, Mingqi Yin, Gian Marco Serra, Lucia Cai
-   Approximately how much time did you spend on this problem set:
    ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute
rule](https://mam202.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}.
You know enough R (and have enough examples of code from class and your
readings) to be able to do this. If you get stuck, ask for help from
others, post a question on Slack-- and remember that I am here to help
too!

> As a true test to yourself, do you understand the code you submitted
> and are you able to explain it to someone else?

# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all
components. Code is poorly written and not documented. Uses the same
type of plot for each graph, or doesn't use plots appropriate for the
variables being analyzed.

Check (3/5): Solid effort. Hits all the elements. No clear mistakes.
Easy to follow (both the code and the output).

Check plus (5/5): Finished all components of the assignment correctly
and addressed both challenges. Code is well-documented (both
self-documented and with additional comments as necessary). Used
tidyverse, instead of base R. Graphs and tables are properly labelled.
Analysis is clear and easy to follow, either because graphs are labeled
clearly or you've written additional text to describe how you interpret
the output.


